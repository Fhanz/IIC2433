{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IIC-2433 Minería de Datos UC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Versiones de librerías, python 3.8.10\n",
    "\n",
    "- numpy 1.20.3\n",
    "- sklearn 1.0.2\n",
    "- nltk 3.7\n",
    "- keras 2.9.0\n",
    "- tensorflow 2.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "X_train_text, Y_train = fetch_20newsgroups(subset=\"train\", remove=('headers', 'footers', 'quotes'), return_X_y=True)\n",
    "X_test_text, Y_test  = fetch_20newsgroups(subset=\"test\", remove=('headers', 'footers', 'quotes'), return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Actividad en clase\n",
    "\n",
    "Usando el algoritmo **MLP**, haga lo siguiente:\n",
    "\n",
    "- Procese el texto del dataset usando el método **tokenize** visto en clases.\n",
    "- Particione el dataset en **tres** particiones. Para esto, deje en train los primeros 10000 ejemplos de X_train_text y almacene los restantes ejemplos de train en una partición de validación.\n",
    "- Haga **padding** sobre las tres particiones usando el método visto en clases.\n",
    "- Cree una red **MLP** con tres capas densas de 256, 128 y 64 neuronas, respectivamente. Agregue una softmax de salida. \n",
    "- Entrene usando la particion de train y validación pasando como parámetro del fit() lo siguiente: (validation_data=(X_val_vect, val_y)). Use early_stopping con patience=3\n",
    "- Evalúe el modelo en la partición de testing usando classification_report.\n",
    "- Cree un nuevo modelo reemplazando la función mean por la función sum.\n",
    "- Reentrene y evalúe su modelo. Comente sus resultados.\n",
    "- Cuanto termine, me avisa para entregarle una **L (logrado)**.\n",
    "- Recuerde que cada L es una décima más en la nota de la asignatura.\n",
    "- Pueden trabajar de a dos.\n",
    "\n",
    "***Tiene hasta el final de la clase.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "classes = np.unique(Y_train)\n",
    "\n",
    "# Load stop-words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize tokenizer\n",
    "# It's also possible to try with a stemmer or to mix a stemmer and a lemmatizer\n",
    "tokenizer = RegexpTokenizer('[\\'a-zA-Z]+')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(document):\n",
    "    words = []\n",
    "\n",
    "    for sentence in sent_tokenize(document):\n",
    "        tokens = [lemmatizer.lemmatize(t.lower()) for t in tokenizer.tokenize(sentence) if t.lower() not in stop_words and len(t) > 2]\n",
    "        words += tokens\n",
    "\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = []\n",
    "val_docs = []\n",
    "test_docs = []\n",
    "\n",
    "for raw_text in X_train_text[:10000]:\n",
    "    text = tokenize(raw_text)\n",
    "    train_docs.append(text)\n",
    "    \n",
    "for raw_text in X_train_text[10000:]:\n",
    "    text = tokenize(raw_text)\n",
    "    val_docs.append(text)\n",
    "    \n",
    "for raw_text in X_test_text:\n",
    "    text = tokenize(raw_text)\n",
    "    test_docs.append(text)\n",
    "\n",
    "train_y = []\n",
    "val_y = []\n",
    "\n",
    "for label in Y_train[:10000]:\n",
    "    train_y.append(label)\n",
    "    \n",
    "for label in Y_train[10000:]:\n",
    "    val_y.append(label)\n",
    "    \n",
    "train_y = np.asarray(train_y)\n",
    "val_y = np.asarray(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_tokens = 50 ## Hyperparameter, input length\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_docs+val_docs+test_docs)\n",
    "\n",
    "## Vectorizing data to keep 50 words per sample.\n",
    "X_train_vect = pad_sequences(tokenizer.texts_to_sequences(train_docs), maxlen=max_tokens, padding=\"post\", truncating=\"post\", value=0.)\n",
    "X_val_vect = pad_sequences(tokenizer.texts_to_sequences(val_docs), maxlen=max_tokens, padding=\"post\", truncating=\"post\", value=0.)\n",
    "X_test_vect  = pad_sequences(tokenizer.texts_to_sequences(test_docs), maxlen=max_tokens, padding=\"post\", truncating=\"post\", value=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 50, 50)            4753900   \n",
      "                                                                 \n",
      " tf.math.reduce_mean (TFOpLa  (None, 50)               0         \n",
      " mbda)                                                           \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               13056     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 20)                1300      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,809,408\n",
      "Trainable params: 4,809,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "\n",
    "inputs = Input(shape=(max_tokens, ))\n",
    "embeddings_layer = Embedding(input_dim=len(tokenizer.index_word)+1, output_dim=50, input_length=max_tokens, trainable=True)\n",
    "dense1 = Dense(256, activation=\"relu\")\n",
    "dense2 = Dense(128, activation=\"relu\")\n",
    "dense3 = Dense(64, activation=\"relu\")\n",
    "dense4 = Dense(len(classes), activation=\"softmax\")\n",
    "\n",
    "x = embeddings_layer(inputs)\n",
    "x = tf.reduce_mean(x, axis=1) \n",
    "x = dense1(x)\n",
    "x = dense2(x)\n",
    "x = dense3(x)\n",
    "outputs = dense4(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 8s 23ms/step - loss: 2.5437 - accuracy: 0.1655 - val_loss: 2.0236 - val_accuracy: 0.2694\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 1.5632 - accuracy: 0.4667 - val_loss: 1.6126 - val_accuracy: 0.4749\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.8906 - accuracy: 0.7134 - val_loss: 1.6082 - val_accuracy: 0.5358\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.5026 - accuracy: 0.8546 - val_loss: 1.8067 - val_accuracy: 0.5327\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.3222 - accuracy: 0.9083 - val_loss: 2.2679 - val_accuracy: 0.5266\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.2481 - accuracy: 0.9350 - val_loss: 2.3300 - val_accuracy: 0.5350\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.1994 - accuracy: 0.9481 - val_loss: 2.6402 - val_accuracy: 0.5358\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.1692 - accuracy: 0.9577 - val_loss: 2.9864 - val_accuracy: 0.5289\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.1621 - accuracy: 0.9588 - val_loss: 2.9123 - val_accuracy: 0.5381\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.1531 - accuracy: 0.9613 - val_loss: 3.1451 - val_accuracy: 0.5327\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.1374 - accuracy: 0.9645 - val_loss: 3.2402 - val_accuracy: 0.5167\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.1506 - accuracy: 0.9619 - val_loss: 3.2974 - val_accuracy: 0.5198\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.1390 - accuracy: 0.9651 - val_loss: 3.4246 - val_accuracy: 0.5297\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.1377 - accuracy: 0.9649 - val_loss: 3.6171 - val_accuracy: 0.5198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4b6c13a8e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_vect, train_y, batch_size=32, epochs=20, callbacks=[callback], validation_data=(X_val_vect, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['alt.atheism',\n",
    " 'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey',\n",
    " 'sci.crypt',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.politics.misc',\n",
    " 'talk.religion.misc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 1s 2ms/step\n",
      "Test Accuracy : 0.4390600106213489\n",
      "\n",
      "Classification Report : \n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.39      0.24      0.30       319\n",
      "           comp.graphics       0.55      0.31      0.39       389\n",
      " comp.os.ms-windows.misc       0.40      0.42      0.41       394\n",
      "comp.sys.ibm.pc.hardware       0.45      0.35      0.39       392\n",
      "   comp.sys.mac.hardware       0.61      0.43      0.50       385\n",
      "          comp.windows.x       0.50      0.57      0.54       395\n",
      "            misc.forsale       0.44      0.53      0.48       390\n",
      "               rec.autos       0.44      0.37      0.40       396\n",
      "         rec.motorcycles       0.29      0.67      0.41       398\n",
      "      rec.sport.baseball       0.62      0.51      0.56       397\n",
      "        rec.sport.hockey       0.72      0.64      0.68       399\n",
      "               sci.crypt       0.59      0.45      0.51       396\n",
      "         sci.electronics       0.34      0.39      0.36       393\n",
      "                 sci.med       0.54      0.39      0.45       396\n",
      "               sci.space       0.40      0.42      0.41       394\n",
      "  soc.religion.christian       0.65      0.42      0.51       398\n",
      "      talk.politics.guns       0.23      0.41      0.29       364\n",
      "   talk.politics.mideast       0.62      0.55      0.58       376\n",
      "      talk.politics.misc       0.26      0.35      0.30       310\n",
      "      talk.religion.misc       0.28      0.22      0.25       251\n",
      "\n",
      "                accuracy                           0.44      7532\n",
      "               macro avg       0.47      0.43      0.44      7532\n",
      "            weighted avg       0.47      0.44      0.44      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "Y_preds = model.predict(X_test_vect).argmax(axis=-1)\n",
    "\n",
    "print(\"Test Accuracy : {}\".format(accuracy_score(Y_test, Y_preds)))\n",
    "print(\"\\nClassification Report : \")\n",
    "print(classification_report(Y_test, Y_preds, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 50, 50)            4753900   \n",
      "                                                                 \n",
      " tf.math.reduce_sum (TFOpLam  (None, 50)               0         \n",
      " bda)                                                            \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               13056     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 20)                1300      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,809,408\n",
      "Trainable params: 4,809,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(max_tokens, ))\n",
    "embeddings_layer = Embedding(input_dim=len(tokenizer.index_word)+1, output_dim=50, input_length=max_tokens, trainable=True)\n",
    "dense1 = Dense(256, activation=\"relu\")\n",
    "dense2 = Dense(128, activation=\"relu\")\n",
    "dense3 = Dense(64, activation=\"relu\")\n",
    "dense4 = Dense(len(classes), activation=\"softmax\")\n",
    "\n",
    "x = embeddings_layer(inputs)\n",
    "x = tf.reduce_sum(x, axis=1) \n",
    "x = dense1(x)\n",
    "x = dense2(x)\n",
    "x = dense3(x)\n",
    "outputs = dense4(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 8s 23ms/step - loss: 1.9866 - accuracy: 0.3981 - val_loss: 1.2060 - val_accuracy: 0.6347\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.6477 - accuracy: 0.8134 - val_loss: 1.2229 - val_accuracy: 0.6499\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.2685 - accuracy: 0.9285 - val_loss: 1.3586 - val_accuracy: 0.6667\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.1701 - accuracy: 0.9567 - val_loss: 1.6015 - val_accuracy: 0.6728\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.1314 - accuracy: 0.9638 - val_loss: 1.7838 - val_accuracy: 0.6674\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.1236 - accuracy: 0.9654 - val_loss: 2.1662 - val_accuracy: 0.6431\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.1356 - accuracy: 0.9618 - val_loss: 2.3048 - val_accuracy: 0.6263\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.1535 - accuracy: 0.9563 - val_loss: 2.6176 - val_accuracy: 0.6225\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.1515 - accuracy: 0.9579 - val_loss: 2.4666 - val_accuracy: 0.6256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4b6b4899a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_vect, train_y, batch_size=32, epochs=20, callbacks=[callback], validation_data=(X_val_vect, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 1s 2ms/step\n",
      "Test Accuracy : 0.5553637812002125\n",
      "\n",
      "Classification Report : \n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.35      0.41      0.38       319\n",
      "           comp.graphics       0.62      0.53      0.57       389\n",
      " comp.os.ms-windows.misc       0.47      0.60      0.53       394\n",
      "comp.sys.ibm.pc.hardware       0.49      0.52      0.50       392\n",
      "   comp.sys.mac.hardware       0.65      0.50      0.56       385\n",
      "          comp.windows.x       0.65      0.62      0.63       395\n",
      "            misc.forsale       0.72      0.71      0.71       390\n",
      "               rec.autos       0.75      0.51      0.61       396\n",
      "         rec.motorcycles       0.67      0.60      0.63       398\n",
      "      rec.sport.baseball       0.71      0.77      0.74       397\n",
      "        rec.sport.hockey       0.84      0.76      0.80       399\n",
      "               sci.crypt       0.87      0.52      0.65       396\n",
      "         sci.electronics       0.56      0.44      0.49       393\n",
      "                 sci.med       0.32      0.75      0.45       396\n",
      "               sci.space       0.56      0.58      0.57       394\n",
      "  soc.religion.christian       0.52      0.65      0.58       398\n",
      "      talk.politics.guns       0.54      0.40      0.46       364\n",
      "   talk.politics.mideast       0.87      0.43      0.57       376\n",
      "      talk.politics.misc       0.33      0.36      0.35       310\n",
      "      talk.religion.misc       0.25      0.23      0.24       251\n",
      "\n",
      "                accuracy                           0.56      7532\n",
      "               macro avg       0.59      0.54      0.55      7532\n",
      "            weighted avg       0.60      0.56      0.56      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_preds = model.predict(X_test_vect).argmax(axis=-1)\n",
    "\n",
    "print(\"Test Accuracy : {}\".format(accuracy_score(Y_test, Y_preds)))\n",
    "print(\"\\nClassification Report : \")\n",
    "print(classification_report(Y_test, Y_preds, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
